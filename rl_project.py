# -*- coding: utf-8 -*-
"""RL Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZAwq2IRR2yJ5lNUzhX15BnYAIiGBuK6c
"""

import numpy as np
import random
import matplotlib.pyplot as plt

# Elevator Environment
class ElevatorEnv:
    def __init__(self, num_floors=10):
        self.num_floors = num_floors
        self.reset()

    def reset(self, start_floor=None, requests=None):
        self.current_floor = random.randint(0, self.num_floors - 1) if start_floor is None else start_floor
        self.requests = random.sample(range(self.num_floors), 3) if requests is None else requests.copy()
        self.visited = []
        return self._get_state()

    def _get_state(self):
        state = np.zeros(self.num_floors)
        state[self.current_floor] = 1
        return state

    def step(self, action):
        reward = -abs(self.current_floor - action)  # Negative distance
        self.current_floor = action
        if action in self.requests:
            self.requests.remove(action)
            self.visited.append(action)
        done = len(self.requests) == 0
        return self._get_state(), reward, done

# Q-Learning Agent
class QLearningAgent:
    def __init__(self, num_floors, lr=0.1, gamma=0.9, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.05):
        self.q_table = np.zeros((num_floors, num_floors))
        self.num_floors = num_floors
        self.lr = lr
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min

    def select_action(self, state, requests):
        current_floor = np.argmax(state)
        if np.random.rand() < self.epsilon:
            return random.choice(requests)
        q_values = self.q_table[current_floor]
        masked_q_values = [q_values[r] for r in requests]
        return requests[np.argmax(masked_q_values)]

    def update(self, state, action, reward, next_state):
        current_floor = np.argmax(state)
        next_floor = np.argmax(next_state)
        best_next_action = np.max(self.q_table[next_floor])
        td_target = reward + self.gamma * best_next_action
        td_error = td_target - self.q_table[current_floor][action]
        self.q_table[current_floor][action] += self.lr * td_error

    def decay_epsilon(self):
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)

# Initialize
env = ElevatorEnv(num_floors=10)
agent = QLearningAgent(num_floors=10)

# Training loop
num_episodes = 5000
reward_list = []
epsilon_list = []

for episode in range(num_episodes):
    state = env.reset()
    done = False
    total_reward = 0

    while not done:
        action = agent.select_action(state, env.requests)
        next_state, reward, done = env.step(action)
        agent.update(state, action, reward, next_state)
        state = next_state
        total_reward += reward

    agent.decay_epsilon()
    reward_list.append(total_reward)
    epsilon_list.append(agent.epsilon)

    # Print every 500 episodes
    if episode % 500 == 0:
        print(f"Episode {episode}: Total Reward = {total_reward:.2f}, Epsilon = {agent.epsilon:.3f}")

print("âœ… Training complete!")

# Plotting rewards and epsilon
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(reward_list)
plt.title("Episode Rewards")
plt.xlabel("Episode")
plt.ylabel("Total Reward")

plt.subplot(1, 2, 2)
plt.plot(epsilon_list)
plt.title("Epsilon Decay")
plt.xlabel("Episode")
plt.ylabel("Epsilon")
plt.tight_layout()
plt.show()

# Function to get optimized path using trained Q-table
def get_optimized_path(starting_floor, requested_floors, q_table):
    current_floor = starting_floor
    path = [current_floor]
    pending_requests = requested_floors.copy()

    while pending_requests:
        q_values = q_table[current_floor]
        masked_q_values = [q_values[f] for f in pending_requests]
        next_floor = pending_requests[np.argmax(masked_q_values)]
        path.append(next_floor)
        pending_requests.remove(next_floor)
        current_floor = next_floor

    return path

# Take input from user
starting_floor = int(input("\nEnter starting floor (0 to 9): "))
requested_floors = list(map(int, input("Enter requested floors (space-separated): ").split()))

# Get and display the optimized path
optimized_path = get_optimized_path(starting_floor, requested_floors, agent.q_table)
print("\nðŸš€ Optimized Path:", optimized_path)